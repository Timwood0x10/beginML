# GANs (生成对抗网络)

## 目录
- [1. GAN 基础概念](#1-gan-基础概念)
  - [1.1 GAN 的基本原理](#11-gan-的基本原理)
  - [1.2 GAN 的数学表示](#12-gan-的数学表示)
  - [1.3 GAN 的训练过程](#13-gan-的训练过程)
- [2. GAN 的主要变体](#2-gan-的主要变体)
  - [2.1 DCGAN (深度卷积生成对抗网络)](#21-dcgan-深度卷积生成对抗网络)
  - [2.2 CGAN (条件生成对抗网络)](#22-cgan-条件生成对抗网络)
  - [2.3 WGAN (Wasserstein GAN)](#23-wgan-wasserstein-gan)
  - [2.4 CycleGAN (循环一致性生成对抗网络)](#24-cyclegan-循环一致性生成对抗网络)
  - [2.5 StyleGAN (风格生成对抗网络)](#25-stylegan-风格生成对抗网络)
- [3. GAN 的训练技巧](#3-gan-的训练技巧)
  - [3.1 训练稳定性问题](#31-训练稳定性问题)
  - [3.2 模式崩溃问题](#32-模式崩溃问题)
  - [3.3 评估指标](#33-评估指标)
- [4. GAN 的应用场景](#4-gan-的应用场景)
  - [4.1 图像生成](#41-图像生成)
  - [4.2 图像转换](#42-图像转换)
  - [4.3 图像超分辨率](#43-图像超分辨率)
  - [4.4 文本到图像生成](#44-文本到图像生成)
- [5. TensorFlow 实现示例](#5-tensorflow-实现示例)
  - [5.1 基础 GAN 实现](#51-基础-gan-实现)
  - [5.2 DCGAN 实现](#52-dcgan-实现)
  - [5.3 条件 GAN 实现](#53-条件-gan-实现)



## 1. Basic Concepts of GAN
### 1.1 Basic Principles of GAN
Generative Adversarial Networks (GANs) were proposed by Ian Goodfellow in 2014 as a deep learning model. GAN consists of two neural networks: a Generator and a Discriminator, which compete against each other to generate data.

- Generator : Attempts to generate realistic data. It receives random noise as input and outputs synthetic data.
- Discriminator : Attempts to distinguish between real data and fake data generated by the generator. It receives data as input and outputs the probability that the data is real.
These two networks improve each other through adversarial training: the generator tries to deceive the discriminator, while the discriminator tries to accurately distinguish between real and fake data.

### Basic Architecture of GANs
```shell
+-------------+     +----------------+     +---------------+
| Random Noise | --> |   Generator    | --> | Generated Data |
|   z ~ p(z)   |     |                |     |               |
+-------------+     +----------------+     +-------+-------+
|
v
+----------------+     +---------------+
Real Data x ~ p_data(x) --> | Discriminator | <-- | Real/Generated? |
|                |     |   D(x) / D(G(z))  |
+----------------+     +---------------+
```

The training process of GAN can be represented as a minimax game:

### GAN Training Process


```shell
+---------------------+     +---------------------+
| Train Discriminator |     | Train Generator     |
|---------------------|     |---------------------|
| 1. Sample real data |     | 1. Generate random noise z |
| 2. Generate fake data |     | 2. Generate fake data G(z) |
| 3. Calculate discriminator loss |     | 3. Calculate generator loss |
| 4. Update discriminator parameters |     | 4. Update generator parameters |
+---------------------+     +---------------------+
^                           |
|                           |
+---------------------------+
Repeat training process
```

### 1.2 Mathematical Representation of GAN
The training process of GAN can be represented as a minimax game:

$$
min_G \max_D V(D, G) = \mathbb{E} {x \sim p {data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]
$$

Where:

- $G$ is the generator network
- $D$ is the discriminator network
- $p_{data}(x)$ is the real data distribution
- $p_z(z)$ is the noise distribution (usually Gaussian distribution)
- $G(z)$ is the data generated by the generator from noise
- $D(x)$ is the probability that the discriminator judges the input data as real
### 1.3 Training Process of GAN
GAN training is usually divided into two alternating steps:

1. Train Discriminator : Fix generator parameters, update discriminator parameters to better distinguish between real and generated data.
2. Train Generator : Fix discriminator parameters, update generator parameters to generate more realistic data to deceive the discriminator. Loss Function Trend

```shell
Loss
^
|
|    /\      /\      /\
|   /  \    /  \    /  \    - Discriminator Loss
|  /    \  /    \  /    \
| /      \/      \/      \
|/                        \
|        /\      /\      /\
|       /  \    /  \    /  \  - Generator Loss
|      /    \  /    \  /    \
|     /      \/      \/      \
+-------------------------------------> Training Epochs
```

```python
# GAN Training Pseudocode
for epoch in range(num_epochs):
    # Train Discriminator
    for _ in range(k):  # Usually k=1, i.e., train discriminator once per iteration
        # Sample from real data distribution
        real_data = sample_real_data(batch_size)
        # Sample from noise distribution
        noise = sample_noise(batch_size)
        # Generate fake data
        fake_data = generator(noise)
        
        # Calculate discriminator loss
        real_loss = -log(discriminator(real_data))
        fake_loss = -log(1 - discriminator(fake_data))
        d_loss = real_loss + fake_loss
        
        # Update discriminator parameters
        update_discriminator(d_loss)
    
    # Train Generator
    noise = sample_noise(batch_size)
    # Calculate generator loss
    g_loss = -log(discriminator(generator(noise)))
    
    # Update generator parameters
    update_generator(g_loss)
```



## 2. Main Variants of GAN
### 2.1 DCGAN (Deep Convolutional GAN)
DCGAN is a variant of GAN that uses convolutional neural networks (CNN) in the generator and discriminator instead of fully connected layers. DCGAN introduces architectural improvements that make GAN training more stable.

Main Features:

- Uses convolution and transposed convolution layers instead of fully connected layers
- Uses Batch Normalization
- Uses ReLU activation in the generator and LeakyReLU in the discriminator
- Removes fully connected layers and pooling layers DCGAN Generator Example

DCGAN Generator Example
```shell
+-------------+     +----------------+     +----------------+
| Random Noise | --> | Dense + BN + ReLU | --> | Reshape to 7x7x256 |
+-------------+     +----------------+     +----------------+
|
v
+----------------+     +----------------+     +----------------+
| 28x28x1 Image  | <-- | Transposed Conv + BN + ReLU | <-- | Transposed Conv + BN + ReLU |
| (tanh activation) |     | 14x14x64        |     | 7x7x128         |
+----------------+     +----------------+     +----------------+
```

```python
# DCGAN Generator Example
def build_generator():
    model = tf.keras.Sequential([
        # Start from 100-dimensional noise
        tf.keras.layers.Dense(7*7*256, use_bias=False, input_shape=(100,)),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.ReLU(),
        
        # Reshape to 7x7x256 feature map
        tf.keras.layers.Reshape((7, 7, 256)),
        
        # Transposed Convolution Layer 1
        tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.ReLU(),
        
        # Transposed Convolution Layer 2
        tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.ReLU(),
        
        # Transposed Convolution Layer 3, output 28x28x1 image
        tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')
    ])
    return model
```

### 2.2 CGAN (Conditional GAN)
Conditional GAN (CGAN) is a variant of GAN that provides additional conditional information (such as class labels) to the generator and discriminator, enabling the model to generate data of specific categories.

Main Features:

- Both generator and discriminator receive conditional information as input
- Can control the category or attributes of generated data
- Improves the diversity and quality of generated data CGAN Generator Example

#### CGAN Generator Example
```shell

+-------------+ | Random Noise | +-------------+\    +----------------+     +---------------+
>-->| Generator | --> | Generated Data |
+-------------+/    | Generator   |     |               |
| Conditional Label |     +----------------+     +-------+-------+
+-------------+                                    |
v
+-------------+     +----------------+     +---------------+
| Real Data   | --> | Discriminator  | <-- | Real/Generated? |
+-------------+\    | Discriminator |     |               |
>-->+----------------+     +---------------+
+-------------+/ | Conditional Label | +-------------+
```

```python
def build_conditional_generator(num_classes=10):
    # Noise Input
    noise = tf.keras.layers.Input(shape=(100,))
    # Conditional Input (Class Label)
    label = tf.keras.layers.Input(shape=(1,), dtype='int32')
    
    # Convert class label to one-hot encoding
    label_embedding = tf.keras.layers.Embedding(num_classes, 50)(label)
    label_embedding = tf.keras.layers.Flatten()(label_embedding)
    
    # Concatenate noise and conditional information
    combined_input = tf.keras.layers.Concatenate()([noise, label_embedding])
    
    # Generator Network
    x = tf.keras.layers.Dense(7*7*256, use_bias=False)(combined_input)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.ReLU()(x)
    
    # Reshape to 7x7x256 feature map
    x = tf.keras.layers.Reshape((7, 7, 256))(x)
    
    # Transposed Convolution Layer
    x = tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.ReLU()(x)
    
    x = tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.ReLU()(x)
    
    # Output 28x28x1 image
    output = tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')(x)
    
    model = tf.keras.Model([noise, label], output)
    return model

```


### 2.3 WGAN (Wasserstein GAN)
Wasserstein GAN (WGAN) was proposed to address the instability and mode collapse issues in GAN training. It uses the Wasserstein distance (also known as Earth Mover's Distance, EMD) instead of the JS divergence used in traditional GANs as the loss function.

Main Features:

- Uses Wasserstein distance as the loss function
- Removes the sigmoid activation function in the discriminator
- Clips the weights of the discriminator (called "critic" in WGAN) to satisfy the Lipschitz constraint
- Provides a more stable training process and more meaningful loss curve

```python
# WGAN Training Pseudocode
for epoch in range(num_epochs):
    # Train Critic
    for _ in range(n_critic):  # Usually n_critic=5
        # Sample from real data distribution
        real_data = sample_real_data(batch_size)
        # Sample from noise distribution
        noise = sample_noise(batch_size)
        # Generate fake data
        fake_data = generator(noise)
        
        # Calculate critic loss
        critic_loss = -tf.reduce_mean(critic(real_data)) + tf.reduce_mean(critic(fake_data))
        
        # Update critic parameters
        update_critic(critic_loss)
        
        # Clip critic weights
        clip_critic_weights(-0.01, 0.01)
    
    # Train Generator
    noise = sample_noise(batch_size)
    # Calculate generator loss
    generator_loss = -tf.reduce_mean(critic(generator(noise)))
    
    # Update generator parameters
    update_generator(generator_loss)
```

### 2.4 CycleGAN (Cycle-Consistent GAN)
CycleGAN is a GAN variant used for image-to-image translation that can learn mappings between two domains without paired data.

Main Features:

- Contains two generators and two discriminators
- Uses cycle consistency loss to ensure consistency of translation
- Does not require paired training data
- Suitable for tasks like style transfer and season conversion


```python
# CycleGAN Cycle Consistency Loss
def cycle_consistency_loss(real_x, real_y, generator_G, generator_F):
    # From X to Y and back to X
    fake_y = generator_G(real_x)
    cycled_x = generator_F(fake_y)
    
    # From Y to X and back to Y
    fake_x = generator_F(real_y)
    cycled_y = generator_G(fake_x)
    
    # Calculate cycle consistency loss
    forward_cycle_loss = tf.reduce_mean(tf.abs(real_x - cycled_x))
    backward_cycle_loss = tf.reduce_mean(tf.abs(real_y - cycled_y))
    
    # Total cycle consistency loss
    total_cycle_loss = forward_cycle_loss + backward_cycle_loss
    
    return total_cycle_loss
 ```


### 2.5 StyleGAN
StyleGAN, proposed by NVIDIA researchers, is a GAN variant that can generate high-quality images and control different levels of style in the generated images.

Main Features:

- Uses a mapping network to map latent space to intermediate latent space
- Uses AdaIN (Adaptive Instance Normalization) to control style
- Introduces noise injection and mixing regularization
- Can generate high-resolution, high-quality images
- Supports style mixing and attribute editing
## 3. Training Techniques for GAN
### 3.1 Training Stability Issues
GAN training is often unstable and may encounter the following issues:

- Imbalance between generator and discriminator
- Gradient vanishing or explosion
- Oscillation during training
Solutions:

- Use more stable GAN variants like WGAN or WGAN-GP
- Use Batch Normalization
- Use appropriate learning rate and optimizer
- Balance training between generator and discriminator
- Use Label Smoothing
- Use Spectral Normalization

```python
# Label Smoothing Example
# Traditional real label is 1.0, can be smoothed to 0.9
real_labels = tf.ones((batch_size, 1)) * 0.9
fake_labels = tf.zeros((batch_size, 1))

# Spectral Normalization Example
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.constraints import max_norm

# Use weight constraint to simulate spectral normalization
conv_layer = Conv2D(64, (3, 3), kernel_constraint=max_norm(1.0))
```


### 3.2 Mode Collapse Issues
Mode collapse is a common issue in GAN training, where the generator only generates a limited variety of samples and fails to cover the entire data distribution.

Causes:

- The generator finds a few modes that can deceive the discriminator
- The discriminator cannot distinguish these modes from real data
Solutions:

- Use Minibatch Discrimination
- Use Wasserstein GAN
- Use diversity-sensitive loss functions
- Use multiple generators or discriminators

```python
# Minibatch Discrimination Example
def minibatch_discrimination(features, num_kernels=5, kernel_dim=3):
    batch_size = tf.shape(features)[0]
    
    # Create a trainable weight matrix
    W = tf.Variable(tf.random.normal([features.shape[1], num_kernels * kernel_dim]))
    
    # Map features to new space
    M = tf.matmul(features, W)
    M = tf.reshape(M, [-1, num_kernels, kernel_dim])
    
    # Calculate L1 distance between samples
    M_expanded = tf.expand_dims(M, 1)
    M_tile = tf.tile(M_expanded, [1, batch_size, 1, 1])
    
    M_expanded_t = tf.expand_dims(M, 0)
    M_tile_t = tf.tile(M_expanded_t, [batch_size, 1, 1, 1])
    
    L1_dist = tf.reduce_sum(tf.abs(M_tile - M_tile_t), axis=3)
    
    # Apply negative exponent
    K = tf.reduce_sum(tf.exp(-L1_dist), axis=1)
    
    # Concatenate to original features
    return tf.concat([features, K], axis=1)
```

### 3.3 Evaluation Metrics
Common metrics for evaluating GAN performance:

1. Inception Score (IS) :
   
   - Measures the quality and diversity of generated images
   - High IS indicates good quality and diversity of generated images
2. Fréchet Inception Distance (FID) :
   
   - Measures the distance between the feature distributions of real and generated images
   - Low FID indicates generated images are closer to the real distribution
3. Precision and Recall :
   
   - Precision: How real the generated samples are
   - Recall: How much of the real distribution the generator covers
```python
# Example of Calculating FID
def calculate_fid(real_images, generated_images, inception_model):
    # Extract features using Inception model
    real_features = inception_model.predict(real_images)
    gen_features = inception_model.predict(generated_images)
    
    # Calculate mean and covariance
    mu_real = np.mean(real_features, axis=0)
    sigma_real = np.cov(real_features, rowvar=False)
    
    mu_gen = np.mean(gen_features, axis=0)
    sigma_gen = np.cov(gen_features, rowvar=False)
    
    # Calculate squared difference of means
    ssdiff = np.sum((mu_real - mu_gen) ** 2.0)
    
    # Calculate distance between covariances
    covmean = sqrtm(sigma_real.dot(sigma_gen))
    
    # Ensure covmean is real
    if np.iscomplexobj(covmean):
        covmean = covmean.real
    
    # Calculate FID
    fid = ssdiff + np.trace(sigma_real + sigma_gen - 2.0 * covmean)
    
    return fid
```

## 4. Application Scenarios of GAN
### 4.1 Image Generation
GANs are initially and most widely used for generating realistic images:

- Face generation
- Artwork creation
- Fictional characters or scenes generation
```python
# Example of generating faces using StyleGAN
import tensorflow as tf
import numpy as np
import PIL.Image

# Load pre-trained StyleGAN model
model = tf.keras.models.load_model('stylegan_model')

# Generate random latent vector
latent_vector = np.random.normal(0, 1, (1, 512))

# Generate image
generated_image = model.predict(latent_vector)

# Display image
PIL.Image.fromarray(((generated_image[0] + 1) * 127.5).astype(np.uint8))
```


### 4.2 Image Transformation
GANs can perform various image transformation tasks:

- Style transfer (e.g., converting photos to Monet-style paintings)
- Image-to-image translation (e.g., colorizing black-and-white photos)
- Domain adaptation (e.g., converting daytime images to nighttime)
```python
# Example of style transfer using CycleGAN
def generate_images(model, test_input):
    prediction = model(test_input)
    
    plt.figure(figsize=(12, 12))
    
    display_list = [test_input[0], prediction[0]]
    title = ['Input Image', 'Output Image']
    
    for i in range(2):
        plt.subplot(1, 2, i+1)
        plt.title(title[i])
        plt.imshow(display_list[i] * 0.5 + 0.5)
        plt.axis('off')
    plt.show()
```

### 4.2 Image Transformation
GANs can perform various image transformation tasks:

- Style transfer (e.g., converting photos to Monet-style paintings)
- Image-to-image translation (e.g., colorizing black-and-white photos)
- Domain adaptation (e.g., converting daytime images to nighttime)
```python
# Example of style transfer using CycleGAN
def generate_images(model, test_input):
    prediction = model(test_input)
    
    plt.figure(figsize=(12, 12))
    
    display_list = [test_input[0], prediction[0]]
    title = ['Input Image', 'Output Image']
    
    for i in range(2):
        plt.subplot(1, 2, i+1)
        plt.title(title[i])
        plt.imshow(display_list[i] * 0.5 + 0.5)
        plt.axis('off')
    plt.show()
```

### 4.3 Image Super-Resolution
GANs can be used to enhance image resolution:

- Convert low-resolution images to high-resolution images
- Restore old photos
- Improve the quality of medical images

```python
# SRGAN (Super-Resolution GAN) Example
def build_srgan_generator():
    # Low-resolution input
    input_layer = tf.keras.layers.Input(shape=(64, 64, 3))
    
    # Initial feature extraction
    x = tf.keras.layers.Conv2D(64, (9, 9), padding='same')(input_layer)
    x = tf.keras.layers.PReLU(shared_axes=[1, 2])(x)
    
    # Residual blocks
    skip = x
    for _ in range(16):
        x = residual_block(x, 64)
    
    x = tf.keras.layers.Conv2D(64, (3, 3), padding='same')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Add()([x, skip])
    
    # Upsampling blocks
    x = upsample_block(x, 256)
    x = upsample_block(x, 256)
    
    # Output high-resolution image
    output_layer = tf.keras.layers.Conv2D(3, (9, 9), padding='same', activation='tanh')(x)
    
    return tf.keras.Model(inputs=input_layer, outputs=output_layer)

def residual_block(x, filters):
    skip = x
    x = tf.keras.layers.Conv2D(filters, (3, 3), padding='same')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.PReLU(shared_axes=[1, 2])(x)
    x = tf.keras.layers.Conv2D(filters, (3, 3), padding='same')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Add()([x, skip])
    return x

def upsample_block(x, filters):
    x = tf.keras.layers.Conv2D(filters, (3, 3), padding='same')(x)
    x = tf.keras.layers.UpSampling2D(size=2)(x)
    x = tf.keras.layers.PReLU(shared_axes=[1, 2])(x)
    return x

```

### 4.4 Text-to-Image Generation
GANs can generate images based on text descriptions:

- Generate product images from descriptions
- Convert text descriptions to scene images
- Assist in creative design and prototyping

```python
# Simplified example of text-to-image GAN
def build_text_to_image_generator(text_embedding_dim=256, noise_dim=100):
    # Text embedding input
    text_input = tf.keras.layers.Input(shape=(text_embedding_dim,))
    
    # Noise input
    noise_input = tf.keras.layers.Input(shape=(noise_dim,))
    
    # Combine text embedding and noise
    combined_input = tf.keras.layers.Concatenate()([text_input, noise_input])
    
    # Fully connected layer
    x = tf.keras.layers.Dense(7*7*256)(combined_input)
    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)
    x = tf.keras.layers.Reshape((7, 7, 256))(x)
    
    # Upsampling layers
    x = tf.keras.layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same')(x)
    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)
    
    x = tf.keras.layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same')(x)
    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)
    
    # Output layer
    output = tf.keras.layers.Conv2D(3, (4, 4), padding='same', activation='tanh')(x)
    
    return tf.keras.Model(inputs=[text_input, noise_input], outputs=output)
```


## 5. TensorFlow Implementation Examples
### 5.1 Basic GAN Implementation
Below is an example of implementing a basic GAN using TensorFlow:

```python
import tensorflow as tf
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import numpy as np

# Load MNIST dataset
(train_images, _), (_, _) = tf.keras.datasets.mnist.load_data()
train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')
train_images = (train_images - 127.5) / 127.5  # Normalize to [-1, 1]

BUFFER_SIZE = 60000
BATCH_SIZE = 256
train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

# Build generator
def make_generator_model():
    model = tf.keras.Sequential()
    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Reshape((7, 7, 256)))
    
    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))

    return model

# Build discriminator
def make_discriminator_model():
    model = tf.keras.Sequential()
    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Flatten())
    model.add(layers.Dense(1))

    return model

# Define loss function and optimizer
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

generator_optimizer = tf.keras.optimizers.Adam(1e-4)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)

# Create models
generator = make_generator_model()
discriminator = make_discriminator_model()

# Training step
@tf.function
def train_step(images):
    noise = tf.random.normal([BATCH_SIZE, 100])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_images = generator(noise, training=True)

        real_output = discriminator(images, training=True)
        fake_output = discriminator(generated_images, training=True)
        
        gen_loss = generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)
    
    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)
    
    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))
    
    return gen_loss, disc_loss

# Training function
def train(dataset, epochs):
    for epoch in range(epochs):
        epoch_gen_loss = 0
        epoch_disc_loss = 0
        num_batches = 0
        
        for image_batch in dataset:
            gen_loss, disc_loss = train_step(image_batch)
            epoch_gen_loss += gen_loss
            epoch_disc_loss += disc_loss
            num_batches += 1
        
        # Print loss for each epoch
        print(f'Epoch {epoch+1}, Generator Loss: {epoch_gen_loss/num_batches}, Discriminator Loss: {epoch_disc_loss/num_batches}')
        
        # Generate and save images
        generate_and_save_images(generator, epoch + 1)

# Generate and save images
def generate_and_save_images(model, epoch):
    # Generate images
    noise = tf.random.normal([16, 100])
    generated_images = model(noise, training=False)
    
    fig = plt.figure(figsize=(4, 4))
    
    for i in range(16):
        plt.subplot(4, 4, i+1)
        plt.imshow(generated_images[i, :, :, 0] * 0.5 + 0.5, cmap='gray')
        plt.axis('off')
    
    plt.savefig(f'gan_image_epoch_{epoch}.png')
    plt.close()

# Train the model
EPOCHS = 50
train(train_dataset, EPOCHS)
```

### 5.2 DCGAN Implementation
Below is an example of implementing DCGAN using TensorFlow:

```python
import tensorflow as tf
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import numpy as np
import time

# Load MNIST dataset
(train_images, _), (_, _) = tf.keras.datasets.mnist.load_data()
train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')
train_images = (train_images - 127.5) / 127.5  # Normalize to [-1, 1]

BUFFER_SIZE = 60000
BATCH_SIZE = 256
train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

# Build DCGAN generator
def make_dcgan_generator():
    model = tf.keras.Sequential()
    
    # First layer, from noise to 7x7x256 feature map
    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
    model.add(layers.BatchNormalization())
    model.add(layers.ReLU())
    model.add(layers.Reshape((7, 7, 256)))
    
    # Second layer, upsample to 7x7x128
    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    model.add(layers.BatchNormalization())
    model.add(layers.ReLU())
    
    # Third layer, upsample to 14x14x64
    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    model.add(layers.BatchNormalization())
    model.add(layers.ReLU())
    
    # Fourth layer, upsample to 28x28x1
    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    
    return model

# Build DCGAN discriminator
def make_dcgan_discriminator():
    model = tf.keras.Sequential()
    
    # First layer, 28x28x1 -> 14x14x64
    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.Dropout(0.3))
    
    # Second layer, 14x14x64 -> 7x7x128
    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.Dropout(0.3))
    
    # Third layer, flatten and output single value
    model.add(layers.Flatten())
    model.add(layers.Dense(1))
    
    return model

# Define loss function and optimizer
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

# Use Adam optimizer with learning rate 2e-4 and beta_1 0.5
generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

# Create models
generator = make_dcgan_generator()
discriminator = make_dcgan_discriminator()

# Training step
@tf.function
def train_step(images):
    noise = tf.random.normal([BATCH_SIZE, 100])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_images = generator(noise, training=True)

        real_output = discriminator(images, training=True)
        fake_output = discriminator(generated_images, training=True)
        
        gen_loss = generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)
    
    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)
    
    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))
    
    return gen_loss, disc_loss

# Training function
def train(dataset, epochs):
    for epoch in range(epochs):
        start = time.time()
        
        epoch_gen_loss = 0
        epoch_disc_loss = 0
        num_batches = 0
        
        for image_batch in dataset:
            gen_loss, disc_loss = train_step(image_batch)
            epoch_gen_loss += gen_loss
            epoch_disc_loss += disc_loss
            num_batches += 1
        
        # Print loss and time for each epoch
        print(f'Epoch {epoch+1}, Generator Loss: {epoch_gen_loss/num_batches}, '
              f'Discriminator Loss: {epoch_disc_loss/num_batches}, '
              f'Time: {time.time()-start} sec')
        
        # Generate and save images every 5 epochs
        if (epoch + 1) % 5 == 0:
            generate_and_save_images(generator, epoch + 1)
    
    # Generate final images after training
    generate_and_save_images(generator, epochs)

# Generate and save images
def generate_and_save_images(model, epoch):
    # Generate images
    noise = tf.random.normal([16, 100])
    generated_images = model(noise, training=False)
    
    fig = plt.figure(figsize=(4, 4))
    
    for i in range(16):
        plt.subplot(4, 4, i+1)
        plt.imshow(generated_images[i, :, :, 0] * 0.5 + 0.5, cmap='gray')
        plt.axis('off')
    
    plt.savefig(f'dcgan_image_epoch_{epoch}.png')
    plt.close()

# Train the model
EPOCHS = 50
train(train_dataset, EPOCHS)
```

### 5.3 Conditional GAN Implementation
Below is an example of implementing Conditional GAN using TensorFlow:

```python
import tensorflow as tf
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import numpy as np
import time

# Load MNIST dataset (including labels)
(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()
train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')
train_images = (train_images - 127.5) / 127.5  # Normalize to [-1, 1]

BUFFER_SIZE = 60000
BATCH_SIZE = 256
NUM_CLASSES = 10

# Create dataset
train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))
train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

# Build Conditional GAN generator
def make_cgan_generator(num_classes=10):
    # Noise input
    noise_input = layers.Input(shape=(100,))
    # Label input
    label_input = layers.Input(shape=(1,), dtype='int32')
    
    # Embed and flatten label
    label_embedding = layers.Embedding(num_classes, 50)(label_input)
    label_embedding = layers.Flatten()(label_embedding)
    
    # Concatenate noise and label
    combined_input = layers.Concatenate()([noise_input, label_embedding])
    
    # Fully connected layer
    x = layers.Dense(7*7*256, use_bias=False)(combined_input)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)
    
    # Reshape to 7x7x256 feature map
    x = layers.Reshape((7, 7, 256))(x)
    
    # Transposed convolution layer
    x = layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)
    
    x = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)
    
    # Output layer
    output = layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')(x)
    
    model = tf.keras.Model([noise_input, label_input], output)
    return model

# Build Conditional GAN discriminator
def make_cgan_discriminator(num_classes=10):
    # Image input
    image_input = layers.Input(shape=(28, 28, 1))
    # Label input
    label_input = layers.Input(shape=(1,), dtype='int32')
    
    # Embed label
    label_embedding = layers.Embedding(num_classes, 50)(label_input)
    label_embedding = layers.Flatten()(label_embedding)
    
    # Reshape label to be compatible with image
    label_dense = layers.Dense(28*28)(label_embedding)
    label_reshaped = layers.Reshape((28, 28, 1))(label_dense)
    
    # Concatenate image and label
    combined_input = layers.Concatenate()([image_input, label_reshaped])
    
    # Convolution layers
    x = layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')(combined_input)
    x = layers.LeakyReLU(alpha=0.2)(x)
    x = layers.Dropout(0.3)(x)
    
    x = layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')(x)
    x = layers.LeakyReLU(alpha=0.2)(x)
    x = layers.Dropout(0.3)(x)
    
    # Output layer
    x = layers.Flatten()(x)
    output = layers.Dense(1)(x)
    
    model = tf.keras.Model([image_input, label_input], output)
    return model

# Define loss function and optimizer
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

# Create models
generator = make_cgan_generator(NUM_CLASSES)
discriminator = make_cgan_discriminator(NUM_CLASSES)

# Training step
@tf.function
def train_step(images, labels):
    batch_size = tf.shape(images)[0]
    noise = tf.random.normal([batch_size, 100])
    random_labels = tf.random.uniform([batch_size, 1], 0, NUM_CLASSES, dtype=tf.int32)

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        # Generate fake images
        generated_images = generator([noise, labels], training=True)

        # Get discriminator outputs for real and fake images
        real_output = discriminator([images, labels], training=True)
        fake_output = discriminator([generated_images, labels], training=True)
        
        # Calculate losses
        gen_loss = generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)
    
    # Calculate gradients
    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)
    
    # Apply gradients
    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))
    
    return gen_loss, disc_loss

# Training function
def train(dataset, epochs):
    for epoch in range(epochs):
        start = time.time()
        
        epoch_gen_loss = 0
        epoch_disc_loss = 0
        num_batches = 0
        
        for image_batch, label_batch in dataset:
            gen_loss, disc_loss = train_step(image_batch, label_batch)
            epoch_gen_loss += gen_loss
            epoch_disc_loss += disc_loss
            num_batches += 1
        
        # Print epoch stats
        print(f'Epoch {epoch+1}, Generator Loss: {epoch_gen_loss/num_batches}, '
              f'Discriminator Loss: {epoch_disc_loss/num_batches}, '
              f'Time: {time.time()-start} sec')
        
        # Generate and save images every 5 epochs
        if (epoch + 1) % 5 == 0:
            generate_and_save_images(generator, epoch + 1)
    
    # Generate final images
    generate_and_save_images(generator, epochs)

# Function to generate and save images
def generate_and_save_images(model, epoch
    # Create a grid of images for each digit (0-9)
    rows = 2
    cols = 5
    num_examples = rows * cols
    
    fig = plt.figure(figsize=(cols*2, rows*2))
    
    # Generate images for each digit
    for i in range(num_examples):
        noise = tf.random.normal([1, 100])
        label = tf.constant([[i % 10]], dtype=tf.int32)
        
        generated_image = model([noise, label], training=False)
        
        plt.subplot(rows, cols, i+1)
        plt.imshow(generated_image[0, :, :, 0] * 0.5 + 0.5, cmap='gray')
        plt.title(f'Digit: {i % 10}')
        plt.axis('off')
    
    plt.savefig(f'cgan_image_epoch_{epoch}.png')
    plt.close()

# Train the model
EPOCHS = 50
train(train_dataset, EPOCHS)
```
